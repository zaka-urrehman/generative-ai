{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import  load_dotenv , find_dotenv\n",
    "import os\n",
    "\n",
    "_ : bool = load_dotenv(find_dotenv())\n",
    "\n",
    "client : OpenAI = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8RWbcdrH36Ozlpg8anZQ8rQTqY2QU', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='I am a language model AI created by OpenAI, designed to assist and provide information on various topics. How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1701570884, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=28, prompt_tokens=11, total_tokens=39))\n",
      "I am a language model AI created by OpenAI, designed to assist and provide information on various topics. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages = [{\n",
    "              \"role\":\"user\" ,\n",
    "              \"content\" : \"who are you?\"\n",
    "              }],\n",
    "    model = \"gpt-3.5-turbo-1106\"\n",
    ")\n",
    "\n",
    "print(response)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "\n",
    "def chat_completion(prompt : str )-> str:\n",
    " response : ChatCompletion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "    )\n",
    "# print(response)\n",
    "#  print(response.choices[0].message.content)\n",
    " return response.choices[0].message.content\n",
    "\n",
    "chat_completion(\"what is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the land of code, where algorithms reign,\n",
      "There lies a concept both simple and arcane,\n",
      "Recursion it's called, a mystical spell,\n",
      "That dances through functions, a tale to tell.\n",
      "\n",
      "Like a mirror reflecting its own reflection,\n",
      "Recursion calls itself, without objection,\n",
      "A function that calls itself, again and again,\n",
      "A loop in disguise, it's quite the brain gain.\n",
      "\n",
      "For when a problem breaks into smaller bits,\n",
      "Recursion steps in, and courage it admits,\n",
      "It dives deep within, solving one by one,\n",
      "Till the puzzle is solved, and the victory won.\n",
      "\n",
      "Like a Russian doll nested in layers of lore,\n",
      "Recursion unwinds, unraveling much more,\n",
      "Till the base case is found, the end of the line,\n",
      "And the code unwinds gracefully, oh so divine.\n",
      "\n",
      "So embrace recursion, with its mesmerizing charm,\n",
      "For in the world of programming, it's more than just a farm,\n",
      "It's a powerful tool, a clever coding friend,\n",
      "That brings elegance and grace to code, from beginning to end.\n"
     ]
    }
   ],
   "source": [
    "def chat_completion()->str:\n",
    "  completion : ChatCompletion = client.chat.completions.create(\n",
    "    model  = \"gpt-3.5-turbo-1106\",\n",
    "    messages= [\n",
    "      {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "print(chat_completion())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This\n",
      " is\n",
      " a\n",
      " test\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n",
    "    stream=True, # mark the stream true to turn on streaming\n",
    ")\n",
    "\n",
    "\n",
    "for part in stream:\n",
    "    print(part.choices[0].delta.content or \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting response in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"months\": [\"April\", \"June\", \"September\", \"November\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  response_format={ \"type\": \"json_object\" }, # Adding response Format as json_object\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},#also don't forget to mention json here\n",
    "    {\"role\": \"user\", \"content\": \"List of months that have 30 days\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parsing the structured string into json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'months': ['April', 'June', 'September', 'November']}\n",
      "['April', 'June', 'September', 'November']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# parse response:\n",
    "obj: dict[str, list[str]] = json.loads(response.choices[0].message.content)\n",
    "\n",
    "# the result is a Python dictionary:\n",
    "print(obj)\n",
    "print(obj['months'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location:str, unit:str=\"fahrenheit\")->str:\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"})\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai.types.chat.chat_completion import ChatCompletionMessage, ChatCompletion\n",
    "# from openai.types.chat.chat_completion import ChatCompletionMessageParam, ChatCompletionMessageParam\n",
    "\n",
    "def run_conversation(main_request: str)->str:\n",
    "    # Step 1: send the conversation and available functions to the model\n",
    "    messages = [{\"role\": \"user\", \"content\": main_request}]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # First Request\n",
    "    response: ChatCompletion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message: ChatCompletionMessage = response.choices[0].message\n",
    "    display(\"* First Response: \", dict(response_message))\n",
    "\n",
    "    tool_calls = response_message.tool_calls\n",
    "    display(\"* First Reponse Tool Calls: \", list(tool_calls))\n",
    "\n",
    "    # Step 2: check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        \n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        \n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(\n",
    "                location=function_args.get(\"location\"),\n",
    "                unit=function_args.get(\"unit\"),\n",
    "            )\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )  # extend conversation with function response\n",
    "        display(\"* Second Request Messages: \", list(messages))\n",
    "        second_response: ChatCompletion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=messages,\n",
    "        )  # get a new response from the model where it can see the function response\n",
    "        print(\"* Second Response: \", dict(second_response))\n",
    "        return second_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'* First Response: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'content': None,\n",
       " 'role': 'assistant',\n",
       " 'function_call': None,\n",
       " 'tool_calls': [ChatCompletionMessageToolCall(id='call_yg8k9Pok0xXI3pV8LLgGV4eI', function=Function(arguments='{\"location\": \"San Francisco\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'),\n",
       "  ChatCompletionMessageToolCall(id='call_NT9ThKfaS8BesOSSWYuEDvwE', function=Function(arguments='{\"location\": \"Tokyo\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'),\n",
       "  ChatCompletionMessageToolCall(id='call_dTijzjRUMJryyWcZMalJZxA5', function=Function(arguments='{\"location\": \"Paris\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function')]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* First Reponse Tool Calls: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='call_yg8k9Pok0xXI3pV8LLgGV4eI', function=Function(arguments='{\"location\": \"San Francisco\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'),\n",
       " ChatCompletionMessageToolCall(id='call_NT9ThKfaS8BesOSSWYuEDvwE', function=Function(arguments='{\"location\": \"Tokyo\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'),\n",
       " ChatCompletionMessageToolCall(id='call_dTijzjRUMJryyWcZMalJZxA5', function=Function(arguments='{\"location\": \"Paris\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* Second Request Messages: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"What's the weather like in San Francisco, Tokyo, and Paris?\"},\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yg8k9Pok0xXI3pV8LLgGV4eI', function=Function(arguments='{\"location\": \"San Francisco\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'), ChatCompletionMessageToolCall(id='call_NT9ThKfaS8BesOSSWYuEDvwE', function=Function(arguments='{\"location\": \"Tokyo\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'), ChatCompletionMessageToolCall(id='call_dTijzjRUMJryyWcZMalJZxA5', function=Function(arguments='{\"location\": \"Paris\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function')]),\n",
       " {'tool_call_id': 'call_yg8k9Pok0xXI3pV8LLgGV4eI',\n",
       "  'role': 'tool',\n",
       "  'name': 'get_current_weather',\n",
       "  'content': '{\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"}'},\n",
       " {'tool_call_id': 'call_NT9ThKfaS8BesOSSWYuEDvwE',\n",
       "  'role': 'tool',\n",
       "  'name': 'get_current_weather',\n",
       "  'content': '{\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"}'},\n",
       " {'tool_call_id': 'call_dTijzjRUMJryyWcZMalJZxA5',\n",
       "  'role': 'tool',\n",
       "  'name': 'get_current_weather',\n",
       "  'content': '{\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"}'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Second Response:  {'id': 'chatcmpl-8RWl9n3dff21OK10RPjIaaIS1PjMj', 'choices': [Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Currently, in San Francisco, the weather is 72°F and partly cloudy. In Tokyo, the weather is 10°C and rainy. In Paris, the weather is 22°C and mostly sunny.', role='assistant', function_call=None, tool_calls=None))], 'created': 1701571475, 'model': 'gpt-3.5-turbo-1106', 'object': 'chat.completion', 'system_fingerprint': 'fp_eeff13170a', 'usage': CompletionUsage(completion_tokens=41, prompt_tokens=169, total_tokens=210)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Currently, in San Francisco, the weather is 72°F and partly cloudy. In Tokyo, the weather is 10°C and rainy. In Paris, the weather is 22°C and mostly sunny.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_conversation(\"What's the weather like in San Francisco, Tokyo, and Paris?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
